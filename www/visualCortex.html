					<div class="subsection">


						<p>
							The code for this project contains the following files, available as a <a href="visualCortex.zip">zip
							archive</a>.
					</div>

					<div class="subsection">
						<p>
							<b>Key files to read:</b>
						</p>

						<table border="0" cellpadding="10">

							<tr>
								<td><code><a href="docs/featureLearner.html">featureLearner.py</a></code></td>
								<td>This file defines an object that has methods to both run K-means unsupervised learning and feature extraction.  You will modify this file in the assignment. Do not change existing function names, however feel free to define helper functions as needed.</td>
							</tr>

							<tr>
								<td><code><a href="docs/classifier.html">classifier.py</a></code><td>This file defines an object for training and testing the logistic regression classifier.  You will modify this file.  Do not change existing function names, however feel free to define helper functions as needed.</td>
							</tr>

							<tr>
								<td><code><a href="docs/evaluator.html">evaluator.py</a></code></td>
								<td>This file contains code to evaluate how your classifier performs on the test set.  You should not need to modify this file but may want to read through the comments to understand how it works.</td>
							</tr>

							<tr>
								<td><code><a href="docs/util.html">util.py</a></code></td>
								<td>This file contains the
								<code>
									Image
								</code> class which will create objects that represent data for each image.  Also contained in this file are several helper methods for viewing features and images.  You should not need to modify this file although you should read through it to understand how it works.</td>
							</tr>
						</table>
					</div>

				<div class="span3">
					<div class="well info faq" style="padding-bottom:0px">
						<p>
							<b>Tasks:</b>
						</p>
						<table class="table table-condensed">
							<tbody>
								<tr>
									<td style="width: 120px"><a href ="#ulearn">Unsupervised Learning</a></td>
								</tr>
								<tr>
									<td style="width: 120px"><a href ="#featextract">Feature Extraction</a></td>
								</tr>
								<tr>
									<td style="width: 120px"><a href ="#slearn">Supervised Learning</a></td>
								</tr>
								<tr>
									<td style="width: 120px"><a href ="#extensions">Extensions</a></td>
								</tr>
							</tbody>
						</table>
					</div>
			
				<div class="span3">
					<div class="well info faq">
						<b>Patches:</b> Images are composed of patches which have been converted to gray-scale followed by standard image preprocessing 
						(similar to the human eye). This includes normalizing them for luminance and contrast as well as further 
						<a href="http://ufldl.stanford.edu/wiki/index.php/Whitening">whitening</a>. 
					</div>
				</div>
				
			</div>
			
				<div class="span3">
					<div class="well info faq">
						<p>
							<b>Numpy: </b>Some Numpy functions that may come in handy:
						</p>
						<ul>
							<li>
								<a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randn.html">numpy.random.randn</a>
							</li>
							<li>
								<a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.argmin.html">numpy.argmin</a>
							</li>
							<li>
								<a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html">numpy.hstack</a> or <a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html">numpy.vstack</a>
							</li>
							<li>
								<a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html">numpy.dot</a>
							</li>
							<li>
								<a href="http://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html">numpy.maximum</a>
							</li>
						</ul>

					</div>
				</div>

			</div>

			<p id = "extensions" class="lead">
				<strong>4. What does this mean?</strong>
			</p>
			<div class="row">
				<div class="span9">
					<div class="subsection">
						<p>
							One commonly held hypothesis is that humans process natural stimuli such as images in multiple layers of representation, 
							starting with the raw sensory stimulus and slowly building higher and higher levels of representation. 
							For example in human vision the raw sensory stimulus received in the retina is passed to a first layer of neurons 
							in the visual cortex. This first layer is hypothesized to have many neurons known as simple cells all of which code 
							for different features in the input. In particular, much evidence points to these simple cells as being edge detectors, 
							i.e. they each code for a specific edge of different orientation and translation within the image. 
							The new representation of what the eye sees becomes the activations of these simple cells which the next part of the brain 
							can turn into higher level understanding like: "I'm looking at a bird."
						</p>

						<p>
							The unsupervised algorithm you implemented was able to learn a feature representation of images similar to the first layer of 
							processing in the visual cortex. In other words, the algorithm wasn't told via supervision or hard coded in any other way 
							to learn to represent an image as it's component edges. Amazingly, that's what the agent found to be the most important features 
							in an image on its own! Unsuprisingly, since our features were similar to the human brain we were quite good at telling the 
							difference between birds and planes. This result suggests the AI community may be approaching an understanding 
							of the learning algorithm used by the human brain.
						</p>
					</div>
				</div>
			</div>

			<br/>
			<p class="lead">
				<strong>References</strong>
			</p>
			<div class="row">
				<div class="span9">

					<div class="subsection">
						<ul>
							<li>
								Learning Feature Representations with K-means, Adam Coates and Andrew Y. Ng. In Neural Networks: Tricks of the Trade, Reloaded, Springer LNCS, 2012.(<a href="http://www.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf">pdf</a>)
							</li>
							<li>
								An Analysis of Single-Layer Networks in Unsupervised Feature Learning, Adam Coates, Honglak Lee, and Andrew Y. Ng. In AISTATS 14, 2011.(<a href="http://www.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf">pdf</a>)
							</li>
							<li>
								Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009. (<a href="http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">pdf</a>)
							</li>
						</ul>
